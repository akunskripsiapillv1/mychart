{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UniChart Finetuning on ChartQA Dataset - BPS\n",
    "\n",
    "Notebook ini menggabungkan semua kode yang diperlukan untuk melakukan fine-tuning model UniChart pada dataset ChartQA. Terdiri dari:\n",
    "\n",
    "- Persiapan dataset\n",
    "- Konfigurasi model\n",
    "- Setup training\n",
    "- Eksekusi training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/media/storage/alif/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# !pip install -q datasets\n",
    "# !pip install -q bitsandbytes-cuda112\n",
    "# !pip install -q accelerate\n",
    "# !pip install -q peft\n",
    "# !pip install -q torch torchvision torchaudio\n",
    "# !pip install -q huggingface-hub\n",
    "# !pip install -q tensorboard\n",
    "# !pip install -q --upgrade transformers pytorch-lightning\n",
    "# !pip install -q rouge-score nltk\n",
    "# !pip install -q pycocoevalcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Import Library ------------------\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from nltk import edit_distance\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig, VisionEncoderDecoderConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Part 1: ChartQA Dataset Class ------------------\n",
    "# Kelas ini menangani loading dan preprocessing dataset ChartQA.\n",
    "added_tokens = []\n",
    "\n",
    "class ChartQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for ChartQA data handling\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_split,\n",
    "        max_length: int,\n",
    "        processor: DonutProcessor = None,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        prompt_end_token: str = None,\n",
    "        task_prefix: str = '<opencqa>',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Filter dataset berdasarkan split\n",
    "        self.dataset = dataset_split\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.prompt_end_token = prompt_end_token \n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.processor = processor\n",
    "        self.prompt_end_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "        self.task_prefix = task_prefix\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # input_tensor: decode gambar dari bytes\n",
    "        image = Image.open(io.BytesIO(sample['image'])).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\", legacy=False).pixel_values\n",
    "        input_tensor = pixel_values.squeeze()\n",
    "\n",
    "        # input_ids: proses string input\n",
    "        processed_parse = self.task_prefix + \" \" + sample['query'] + \" \" + self.prompt_end_token + \" \" + sample['label'] + self.processor.tokenizer.eos_token\n",
    "        input_ids = self.processor.tokenizer(\n",
    "            processed_parse,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = self.ignore_id                  # model doesn't need to predict pad token\n",
    "            labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id    # model doesn't need to predict prompt \n",
    "            return input_tensor, input_ids, labels\n",
    "        else:\n",
    "            prompt_end_index = torch.nonzero(input_ids == self.prompt_end_token_id).sum()             # return prompt end index instead of target output labels\n",
    "            return input_tensor, input_ids, prompt_end_index, processed_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Part 2: ChartQA Lightning Module ------------------\n",
    "# Modul PyTorch Lightning yang menangani proses training.\n",
    "class ChartQAModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model, args, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.args = args\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.smoother = SmoothingFunction()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, decoder_input_ids, labels = batch\n",
    "        \n",
    "        outputs = self.model(pixel_values,\n",
    "                             decoder_input_ids=decoder_input_ids[:, :-1],\n",
    "                             labels=labels[:, 1:])\n",
    "        loss = outputs.loss\n",
    "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def extract_numbers(self, text):\n",
    "        \"\"\"Extract all numbers from text as list of ints\"\"\"\n",
    "        return list(map(int, re.findall(r'\\d+', str(text))))\n",
    "\n",
    "    def compute_relaxed_accuracy(self, gt_text, pred_text, tolerance=0.05):\n",
    "        \"\"\"Compare numbers in gt and pred with tolerance (can handle different lengths)\"\"\"\n",
    "        gt_numbers = self.extract_numbers(gt_text)\n",
    "        pred_numbers = self.extract_numbers(pred_text)\n",
    "\n",
    "        matched = 0\n",
    "        used_indices = set()\n",
    "\n",
    "        for gt in gt_numbers:\n",
    "            for i, pred in enumerate(pred_numbers):\n",
    "                if i in used_indices:\n",
    "                    continue\n",
    "                if abs(gt - pred) / max(gt, 1) <= tolerance:\n",
    "                    matched += 1\n",
    "                    used_indices.add(i)\n",
    "                    break  # Match found, move to next ground truth number\n",
    "\n",
    "        return matched / len(gt_numbers) if gt_numbers else 1.0  # Avoid division by zero\n",
    "\n",
    "    def compute_metric(self, gt, pred):\n",
    "        metrics = {}\n",
    "\n",
    "        # 1. Relaxed Accuracy\n",
    "        try:\n",
    "            metrics['relaxed_acc'] = self.compute_relaxed_accuracy(gt, pred)\n",
    "        except:\n",
    "            metrics['relaxed_acc'] = 0.0\n",
    "\n",
    "        # 2. BLEU Score\n",
    "        try:\n",
    "            reference = [str(gt).lower().split()]\n",
    "            candidate = str(pred).lower().split()\n",
    "            metrics['bleu'] = sentence_bleu(\n",
    "                reference,\n",
    "                candidate,\n",
    "                smoothing_function=self.smoother.method4\n",
    "            )\n",
    "        except:\n",
    "            metrics['bleu'] = 0.0\n",
    "\n",
    "        # 3. ROUGE Score\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(str(gt).lower(), str(pred).lower())\n",
    "            metrics['rougeL'] = scores['rougeL'].fmeasure\n",
    "        except:\n",
    "            metrics['rougeL'] = 0.0\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
    "\n",
    "        decoder_prompts = pad_sequence(\n",
    "            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_prompts,\n",
    "            max_length=self.args.max_length,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=4,\n",
    "            bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "        predictions = [\n",
    "            seq.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            .replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            .strip()\n",
    "            for seq in self.processor.tokenizer.batch_decode(outputs.sequences)\n",
    "        ]\n",
    "\n",
    "        # Process predictions and answers\n",
    "        processed_preds = []\n",
    "        processed_answers = []\n",
    "        relaxed_accuracies = []\n",
    "        bleus = []\n",
    "        rougeLs = []\n",
    "\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            # Ekstraksi jawaban model\n",
    "            pred_parts = pred.split(\"<s_answer>\")\n",
    "            pred_part = pred_parts[1].strip() if len(pred_parts) >= 2 else \"\"\n",
    "\n",
    "            # Bersihkan token\n",
    "            pred_part = pred_part.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
    "\n",
    "            # Jawaban ground truth\n",
    "            answer_parts = answer.split(\"<s_answer>\")\n",
    "            answer_part = answer_parts[1].strip() if len(answer_parts) >= 2 else \"\"\n",
    "\n",
    "            # Simpan hasil bersih\n",
    "            processed_preds.append(pred_part)\n",
    "            processed_answers.append(answer_part)\n",
    "\n",
    "            # Hitung metrik\n",
    "            metrics = self.compute_metric(answer_part, pred_part)\n",
    "            relaxed_accuracies.append(metrics['relaxed_acc'])\n",
    "            bleus.append(metrics['bleu'])\n",
    "            rougeLs.append(metrics['rougeL'])\n",
    "\n",
    "        # Log metrik rata-rata per batch\n",
    "        self.log_dict({\n",
    "            \"val_relaxed_acc\": sum(relaxed_accuracies) / len(relaxed_accuracies),\n",
    "            \"val_bleu\": sum(bleus) / len(bleus),\n",
    "            \"val_rougeL\": sum(rougeLs) / len(rougeLs),\n",
    "        }, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        return {\n",
    "            \"preds\": processed_preds,\n",
    "            \"answers\": processed_answers,\n",
    "            \"relaxed_acc\": relaxed_accuracies,\n",
    "            \"bleu\": bleus,\n",
    "            \"rougeL\": rougeLs\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        max_iter = None\n",
    "\n",
    "        if int(self.config.get(\"max_epochs\", -1)) > 0:\n",
    "            assert len(self.config.get(\"train_batch_sizes\")) == 1, \"Set max_epochs hanya jika jumlah dataset adalah 1\"\n",
    "            max_iter = (self.config.get(\"max_epochs\") * self.config.get(\"num_training_samples_per_epoch\")) / (\n",
    "                self.config.get(\"train_batch_sizes\")[0] * torch.cuda.device_count() * self.config.get(\"num_nodes\", 1)\n",
    "            )\n",
    "\n",
    "        if int(self.config.get(\"max_steps\", -1)) > 0:\n",
    "            max_iter = min(self.config.get(\"max_steps\"), max_iter) if max_iter is not None else self.config.get(\"max_steps\")\n",
    "\n",
    "        assert max_iter is not None\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "        scheduler = {\n",
    "            \"scheduler\": self.cosine_scheduler(optimizer, max_iter, self.config.get(\"warmup_steps\")),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_scheduler(optimizer, training_steps, warmup_steps):\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < warmup_steps:\n",
    "                return current_step / max(1, warmup_steps)\n",
    "            progress = current_step - warmup_steps\n",
    "            progress /= max(1, training_steps - warmup_steps)\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "        return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.batch_size, shuffle=True, num_workers=self.args.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.args.valid_batch_size, shuffle=False, num_workers=self.args.num_workers)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        save_path = os.path.join(self.config['result_path'], 'chartqa-checkpoint-epoch=' + str(self.current_epoch) + '-' + str(self.global_step))\n",
    "        # self.model.save_pretrained(save_path)\n",
    "        # self.processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Part 3: Training Configuration ------------------\n",
    "# Mengatur parameter yang awalnya diberikan melalui ArgumentParser\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path = \"akunskripsiapillv1/indochart-v2-dataset\"  # Path ke file data\n",
    "        self.output_dir = \"/media/storage/alif/checkpoints\"  # Path untuk menyimpan checkpoint\n",
    "        self.max_epochs = 2  # Jumlah maksimum epoch\n",
    "        self.batch_size = 2  # Batch size untuk training\n",
    "        self.valid_batch_size = 2  # Batch size untuk validasi\n",
    "        self.max_length = 512  # Maksimal panjang untuk decoder generation\n",
    "        self.num_workers = 2  # Jumlah worker\n",
    "        self.lr = 5e-5  # Learning rate\n",
    "        self.check_val_every_n_epoch = 1  # Validasi setiap n epoch\n",
    "        self.log_every_n_steps = 50  # Logging setiap n steps\n",
    "        self.warmup_steps = 50  # Jumlah warmup steps\n",
    "        self.checkpoint_steps = 1000  # Interval checkpoint\n",
    "        self.gradient_clip_val = 1.0  # Nilai clip gradient\n",
    "        self.accumulate_grad_batches = 1  # Akumulasi grad batches\n",
    "        self.gpus_num = 1  # Jumlah GPU\n",
    "        self.nodes_num = 1  # Jumlah nodes\n",
    "        self.checkpoint_path = \"ahmed-masry/unichart-base-960\"  # Path checkpoint\n",
    "        self.save_every_n_epochs = 1  # Simpan checkpoint setiap n epoch\n",
    "\n",
    "# Buat instance args\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor and model from ahmed-masry/unichart-base-960...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    960,\n",
      "    960\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57531\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from akunskripsiapillv1/indochart-v2-dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268217be6222457191a60ed62eda4a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/29423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a616212937542828a9e80a26f2fa837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333fe98d25544bec8777c14ec8cfab00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training BPS: 9424\n",
      "Jumlah data validasi BPS: 1178\n",
      "Jumlah data test BPS: 1178\n",
      "Preparing training dataset...\n",
      "Preparing validation dataset...\n",
      "Initializing ChartQA module...\n",
      "Setting up PyTorch Lightning trainer...\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Part 4: Main Training Function ------------------\n",
    "# Buat direktori output jika belum ada\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "# Load processor dan model\n",
    "print(f\"Loading processor and model from {args.checkpoint_path}...\")\n",
    "processor = DonutProcessor.from_pretrained(args.checkpoint_path)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(args.checkpoint_path)\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset from {args.data_path}...\")\n",
    "dataset = load_dataset(args.data_path)\n",
    "\n",
    "# --- Filter dataset untuk source='bps' ---\n",
    "def filter_bps(example):\n",
    "    \"\"\"Filter data yang memiliki source='bps'\"\"\"\n",
    "    return example[\"source\"] == \"bps\"\n",
    "\n",
    "# Terapkan filter ke semua split\n",
    "train_bps = dataset[\"train\"].filter(filter_bps)\n",
    "val_bps = dataset[\"validation\"].filter(filter_bps)\n",
    "test_bps = dataset[\"test\"].filter(filter_bps)\n",
    "\n",
    "# Verifikasi hasil filter\n",
    "print(f\"Jumlah data training BPS: {len(train_bps)}\")\n",
    "print(f\"Jumlah data validasi BPS: {len(val_bps)}\")\n",
    "print(f\"Jumlah data test BPS: {len(test_bps)}\")\n",
    "\n",
    "# Persiapkan dataset training\n",
    "print(\"Preparing training dataset...\")\n",
    "train_dataset = ChartQADataset(\n",
    "    train_bps,  # Menggunakan split train\n",
    "    max_length=args.max_length,\n",
    "    processor=processor,\n",
    "    split=\"train\",\n",
    "    prompt_end_token=\"<s_answer>\",\n",
    "    task_prefix=\"<opencqa>\"\n",
    ")\n",
    "\n",
    "# Persiapkan dataset validasi\n",
    "print(\"Preparing validation dataset...\")\n",
    "val_dataset = ChartQADataset(\n",
    "    val_bps,  # Menggunakan split validasi\n",
    "    max_length=args.max_length,\n",
    "    processor=processor,\n",
    "    split=\"validation\",\n",
    "    prompt_end_token=\"<s_answer>\",\n",
    "    task_prefix=\"<opencqa>\"\n",
    ")\n",
    "\n",
    "# Konfigurasi training\n",
    "config = {\n",
    "    \"max_epochs\": args.max_epochs,\n",
    "    \"check_val_every_n_epoch\": args.check_val_every_n_epoch,\n",
    "    \"log_every_n_steps\": args.log_every_n_steps,\n",
    "    \"gradient_clip_val\": args.gradient_clip_val,\n",
    "    \"num_training_samples_per_epoch\": len(dataset[\"train\"]),\n",
    "    \"lr\": args.lr,\n",
    "    \"train_batch_sizes\": [args.batch_size],\n",
    "    \"val_batch_sizes\": [args.valid_batch_size],\n",
    "    \"num_nodes\": args.nodes_num,\n",
    "    \"warmup_steps\": args.warmup_steps,\n",
    "    \"result_path\": args.output_dir,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "# Inisialisasi module ChartQA\n",
    "print(\"Initializing ChartQA module...\")\n",
    "model_module = ChartQAModule(config, processor, model, args, train_dataset, val_dataset)\n",
    "    \n",
    "# Setup checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=args.output_dir, \n",
    "    every_n_epochs=args.save_every_n_epochs, \n",
    "    save_last=False, \n",
    "    save_top_k=-1\n",
    ")\n",
    "\n",
    "# Create CSV logger\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir=args.output_dir,\n",
    "    name=\"unichart-bps-v2\",\n",
    "    version=None\n",
    ")\n",
    "\n",
    "# Inisialisasi trainer PyTorch Lightning\n",
    "print(\"Setting up PyTorch Lightning trainer...\")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=args.gpus_num,\n",
    "    max_epochs=args.max_epochs,\n",
    "    check_val_every_n_epoch=args.check_val_every_n_epoch,\n",
    "    log_every_n_steps=args.log_every_n_steps,\n",
    "    gradient_clip_val=args.gradient_clip_val,\n",
    "    num_nodes=args.nodes_num,\n",
    "    precision=\"16-mixed\",\n",
    "    default_root_dir=args.output_dir,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/storage/alif/mychart/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /media/storage/alif/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                      | Params | Mode\n",
      "-----------------------------------------------------------\n",
      "0 | model | VisionEncoderDecoderModel | 201 M  | eval\n",
      "-----------------------------------------------------------\n",
      "201 M     Trainable params\n",
      "0         Non-trainable params\n",
      "201 M     Total params\n",
      "807.433   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "484       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeb0db95e4647bfb4493b082ff027dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/storage/alif/mychart/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffd6592dd64481cbd00ee10b2372368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a856b28a3304bd58ae700076acd8aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd1926a64bc41d391637ef1990a5236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2542.10 seconds\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Mulai proses training\n",
    "import time\n",
    "print(\"Starting training...\")\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model_module)\n",
    "end = time.time()\n",
    "\n",
    "duration = end - start\n",
    "print(f\"Training time: {duration:.2f} seconds\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T17:50:15.851123Z",
     "iopub.status.busy": "2025-03-25T17:50:15.850595Z",
     "iopub.status.idle": "2025-03-25T17:50:17.551002Z",
     "shell.execute_reply": "2025-03-25T17:50:17.550073Z",
     "shell.execute_reply.started": "2025-03-25T17:50:15.851080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a122ac72e8bb4e6fa1c9c95975fd8698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2c253081b24b1fafae8c4c8a9df973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/akunskripsiapillv1/finetuned-unichart-bps-v2/commit/abcc2520793125dbf68e1d2c92dbcca9cdb3ffc0', commit_message='Upload processor', commit_description='', oid='abcc2520793125dbf68e1d2c92dbcca9cdb3ffc0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/akunskripsiapillv1/finetuned-unichart-bps-v2', endpoint='https://huggingface.co', repo_type='model', repo_id='akunskripsiapillv1/finetuned-unichart-bps-v2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, login\n",
    "\n",
    "hf_token=\"HF_TOKEN\"\n",
    "login(hf_token)\n",
    "\n",
    "# Define model and processor\n",
    "model = model_module.model\n",
    "processor = model_module.processor\n",
    "\n",
    "# Define the repository name where you want to upload the model\n",
    "repo_name = \"akunskripsiapillv1/finetuned-unichart-bps-v2\"\n",
    "\n",
    "# Create the repo (if it doesn't exist)\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "# Push model and processor to Hugging Face Hub\n",
    "model.push_to_hub(repo_name)\n",
    "processor.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def generate_chart_description(model_name: str, image_path: str, input_prompt: str) -> str:\n",
    "    # Load model dan processor\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_name, token=hf_token)\n",
    "    processor = DonutProcessor.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load dan proses gambar\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Tokenisasi prompt\n",
    "    decoder_input_ids = processor.tokenizer(input_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=4,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Decode hasil output\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    result = sequence.split(\"<s_answer>\")[1].strip() if \"<s_answer>\" in sequence else sequence.strip()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Parameter\n",
    "# model_name = \"akunskripsiapillv1/finetuned-unichart-bps-v2\"\n",
    "# image_path = \"/media/storage/alif/mychart/13308.png\"\n",
    "# input_prompt = \"<opencqa> Buatkan deskripsi dari grafik berikut ini secara lengkap dan informatif <s_answer>\"\n",
    "\n",
    "# # Tampilkan gambar\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # Generate deskripsi\n",
    "# description = generate_chart_description(model_name, image_path, input_prompt)\n",
    "# print(\"Deskripsi Hasil Generate:\")\n",
    "# print(description)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
